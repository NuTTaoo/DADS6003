{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPRTvGovKaRKPtJARLZGfy8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NuTTaoo/DADS6003/blob/main/DADS6003_6620422031_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "atQa8ZdbiRi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD_Stochastic GD"
      ],
      "metadata": {
        "id": "Pc6BVGqKdxa1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmyXApeviI_4",
        "outputId": "dbcde7cc-5a7e-44b1-f033-37a763d4bb0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First J =  88.37438854095711\n",
            "Converged, iterations: 839 / 1000\n",
            "35.999999999884814 35.99999999988454\n",
            "[[ 7.]\n",
            " [15.]\n",
            " [-6.]]\n",
            "y predict =  [[13.]]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[0,1],[2,6],[3,8]])\n",
        "y = np.array([1,1,4])\n",
        "x_b = np.concatenate([np.ones((len(x),1)),x],axis=1)\n",
        "theta = np.random.random((x_b.shape[1],1))\n",
        "N = x.shape[0]\n",
        "\n",
        "def cost_function(theta, x_b, y, N):\n",
        "  y_hat = x_b.dot(theta)\n",
        "  c = np.sum(np.square(y_hat - y))\n",
        "  return c\n",
        "def gradient(x_b,y,theta,N):\n",
        "    y_hat = x_b.dot(theta)\n",
        "    grad = x_b.T.dot(y_hat - y)\n",
        "    return grad\n",
        "def stochasti_gradient_descent(x_b,y,theta,learning_rate=0.01,max_iter=1,N=3,ep = 0.001):\n",
        "    J = cost_function(theta, x_b, y, N)\n",
        "    print(\"First J = \",J)\n",
        "\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    while not converged:\n",
        "      round = 1\n",
        "      #print(f'theta เริ่มต้น\\n {theta}')\n",
        "      #print('------------------------------------------------------------------')\n",
        "      for i in range (max_iter):\n",
        "        rand_index = np.random.randint(0,N)\n",
        "        #print(f'Round = {round}')\n",
        "        #print(f'Row {rand_index+1}')\n",
        "        xi = np.array(x_b[rand_index]).reshape(1,-1)\n",
        "        yi = np.array(y[rand_index]).reshape(1,-1)\n",
        "        #print(xi , yi)\n",
        "        grad = gradient(xi,yi,theta,N)\n",
        "        #print(f'Grad = {round}')\n",
        "        #print(grad)\n",
        "        #print('------------------------------------------------------------------')\n",
        "        theta = theta - learning_rate * grad\n",
        "        #print(f'theta = {round}')\n",
        "        #print(theta)\n",
        "        #print('------------------------------------------------------------------')\n",
        "        round += 1\n",
        "        J2 = cost_function(theta, x_b, y, N)\n",
        "\n",
        "      if abs(J-J2) <= ep:\n",
        "        print(f'Converged, iterations: {iter} / {max_iter}')\n",
        "        print(J,J2)\n",
        "        converged = True\n",
        "\n",
        "      J = J2   # update error s\n",
        "      iter += 1  # update iter\n",
        "\n",
        "      if iter == max_iter:\n",
        "        print('       Max iterations exceeded!')\n",
        "        print(J,J2)\n",
        "        converged = True\n",
        "\n",
        "    return theta\n",
        "theta = stochasti_gradient_descent(x_b,y,theta,learning_rate=0.01,max_iter=1000,N=3,ep = 0.000000000001)\n",
        "print(theta)\n",
        "xtest = np.array([[4,9]])\n",
        "xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "y_p = xtest_b.dot(theta) #y_hat\n",
        "print(\"y predict = \", y_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini-batch GD"
      ],
      "metadata": {
        "id": "lDev-9_weAis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[0,1],[2,6],[3,8]])\n",
        "y = np.array([1,1,4])\n",
        "x_b = np.concatenate([np.ones((len(x),1)),x],axis=1)\n",
        "theta = np.random.random((x_b.shape[1],1))\n",
        "N = x.shape[0]\n",
        "batch_size = 2\n",
        "def cost_function_M(theta_M, x_b, y, N):\n",
        "  y_hat = x_b.dot(theta_M)\n",
        "  c = (1/batch_size)*np.sum(np.square(y_hat - y))\n",
        "  return c\n",
        "def gradient_b(X_batch,y_batch,theta_M,N):\n",
        "  y_hat = X_batch.dot(theta_M)\n",
        "  grad = (1/batch_size)*X_batch.T.dot(y_hat - y_batch)\n",
        "  return grad\n",
        "\n",
        "def MiniB_gradient_descent(x_b,y,theta_M,learning_rate=0.01,max_iter=10,N=3,ep = 0.000000000001):\n",
        "  J = cost_function_M(theta_M, x_b, y, N)\n",
        "  print(\"First J = \",J)\n",
        "  converged = False\n",
        "  iter = 0\n",
        "  while not converged:\n",
        "    round = 1\n",
        "    for epoch in range(max_iter):\n",
        "      indices = np.random.permutation(N)\n",
        "      X_shuffled = x_b[indices]\n",
        "      y_shuffled = y[indices].reshape(-1, 1)\n",
        "      num_full_batches = N // batch_size\n",
        "      #print(num_full_batches)\n",
        "      for i in range(0, num_full_batches * batch_size, batch_size):\n",
        "        X_batch = X_shuffled[i:i+batch_size]\n",
        "        y_batch = y_shuffled[i:i+batch_size]\n",
        "        #print(f'round = {round}')\n",
        "        #print(X_shuffled)\n",
        "        #print(y_shuffled)\n",
        "        #print(f'\\nPrint x_batch \\n {X_batch}')\n",
        "        #print(y_batch)\n",
        "        grad = gradient_b(X_batch,y_batch,theta_M,N)\n",
        "        theta_M = theta_M - learning_rate * grad\n",
        "        round += 1\n",
        "        J2 = cost_function_M(theta_M, x_b, y, N)\n",
        "\n",
        "        #print(theta_M)\n",
        "        #print(grad)\n",
        "        #print(J,J2)\n",
        "    if abs(J-J2) <= ep:\n",
        "        print(f'Converged, iterations: {iter} / {max_iter}')\n",
        "        print(J,J2)\n",
        "        converged = True\n",
        "\n",
        "    J = J2   # update error s\n",
        "    iter += 1  # update iter\n",
        "\n",
        "    if iter == max_iter:\n",
        "      print('       Max iterations exceeded!')\n",
        "      print(J,J2)\n",
        "      converged = True\n",
        "\n",
        "  return theta_M\n",
        "\n",
        "theta_M = MiniB_gradient_descent(x_b,y,theta,learning_rate=0.01,max_iter=100000,N=3,ep = 0.000000000001)\n",
        "print(theta_M)\n",
        "xtest = np.array([[4,9]])\n",
        "xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "y_p = xtest_b.dot(theta_M)\n",
        "print(\"y predict = \", y_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrookzSqatsm",
        "outputId": "79a733be-9bf4-419f-96b3-0f71a50de188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First J =  13.901378034497263\n",
            "Converged, iterations: 10 / 100000\n",
            "17.99999999999899 17.999999999999034\n",
            "[[ 7.]\n",
            " [15.]\n",
            " [-6.]]\n",
            "y predict =  [[13.]]\n"
          ]
        }
      ]
    }
  ]
}